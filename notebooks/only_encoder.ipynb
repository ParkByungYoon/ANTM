{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyArgs: \n",
    "    pass\n",
    "\n",
    "args = EmptyArgs()\n",
    "\n",
    "# General arguments\n",
    "args.data_folder='dataset/'\n",
    "args.log_dir='log'\n",
    "args.seed=21\n",
    "args.epochs=1\n",
    "args.gpu_num=0\n",
    "\n",
    "# Model specific arguments\n",
    "args.model_type='GTM'\n",
    "args.use_trends=1\n",
    "args.use_img=1\n",
    "args.use_text=1\n",
    "args.trend_len=52\n",
    "args.num_trends=3\n",
    "args.batch_size=128\n",
    "args.embedding_dim=32\n",
    "args.hidden_dim=64\n",
    "args.output_dim=12\n",
    "args.use_encoder_mask=1\n",
    "args.autoregressive=0\n",
    "args.num_attn_heads=4\n",
    "args.num_hidden_layers=1\n",
    "\n",
    "# wandb arguments\n",
    "args.wandb_entity='bonbak'\n",
    "args.wandb_proj='sflab-gtm'\n",
    "args.wandb_run='Run1'\n",
    "\n",
    "args.sales_total_len=12\n",
    "args.only_4weeks_loss='store_true'\n",
    "\n",
    "args.use_encoder_mask = False\n",
    "args.trend_len = 64  # 52\n",
    "args.prepo_data_folder = \"/home/smart01/SFLAB/sanguk/mind_br_data_prepro/\"\n",
    "args.data_folder = \"/home/smart01/SFLAB/sanguk/mind_br_data/\"\n",
    "args.text_embedder = 'klue/bert-base'\n",
    "args.sales_total_len = 12 # 52  # 12\n",
    "args.seq_len = args.sales_total_len\n",
    "args.output_dim = args.sales_total_len\n",
    "args.autoregressive = 1\n",
    "args.scaler = \"standard\" # \"Minmax\"\n",
    "args.learning_rate = 0.0001\n",
    "args.lead_time = 2\n",
    "args.no_scaling = False\n",
    "args.ahead_step = 6\n",
    "args.val_output_week = 12\n",
    "args.val_output_month = 3\n",
    "args.num_attn_heads = 8\n",
    "args.hidden_dim = 512\n",
    "args.embedding_dim = 256\n",
    "args.only_4weeks_loss = False # True\n",
    "args.num_hidden_layers = 2\n",
    "\n",
    "# args.model_type = \"ANTM_5epoch---231101-0845_nonautoregressive\"\n",
    "args.model_type = \"encoder_boxcox_train_plain_loss_test11_afterencoder_meta_200epoch_0103dataset_optimallmda_centering_2z\"\n",
    "args.autoregressive_train = False\n",
    "args.teacher_forcing = True\n",
    "args.epochs = 1000 # 500  # 300  # 150  # 500 # 50 # 300  # 5  # 500  # 100 # 5  # 100\n",
    "args.batch_size = 16  # 64\n",
    "args.gpu_num = 2\n",
    "args.mean_scalers = True\n",
    "args.mean_sensi = 1 # 2\n",
    "args.std_sensi = 1 # 1.2\n",
    "args.mean_sensi_inference =  1  # 2\n",
    "args.std_sensi_inference =  1  # 1.2\n",
    "args.before_meta = False # True #\n",
    "\n",
    "# 1.2. 기준 optimal lamda\n",
    "# args.mu_lmda = 0.0616\n",
    "# args.std_lmda = 0.1258\n",
    "# args.mu_centering = 0  # 0.75\n",
    "# args.std_centering = 0  # 0.6\n",
    "\n",
    "# 1.2.기준 my lamda\n",
    "# args.mu_lmda =  0.19\n",
    "# args.std_lmda =  0.24\n",
    "# args.mu_centering =  0.5\n",
    "# args.std_centering = 0.4\n",
    "\n",
    "# 1.3. 기준(train에만 일방적으로 추가)\n",
    "args.mu_lmda = 0.0619\n",
    "args.std_lmda = 0.1241\n",
    "args.mu_centering = 0.75\n",
    "args.std_centering = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/smart01/SFLAB/su_GTM_t/GTM_T_sanguk/'\n",
    "\n",
    "meta_df = pd.read_csv(os.path.join(data_path,'240109_all_meta_sales_total.csv'))\n",
    "\n",
    "with open(os.path.join(data_path, '12salesweek_test_item_number296.pkl'), 'rb') as f:\n",
    "    test_ids = pickle.load(f)\n",
    "test_ids = test_ids.drop('MTPT6102')\n",
    "\n",
    "train_df = meta_df[~meta_df['item_number'].isin(test_ids)] \n",
    "test_df = meta_df[meta_df['item_number'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDataset(Dataset):\n",
    "    def __init__(self, df, opt_lambda=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.fab = torch.Tensor(self.df.loc[:,self.df.columns.str.startswith('fabric')].values)\n",
    "        self.cat = torch.Tensor(self.df.loc[:,self.df.columns.str.startswith('category')].values)\n",
    "        self.col = torch.Tensor(self.df.loc[:,self.df.columns.str.startswith('color')].values)\n",
    "        self.img = torch.Tensor(self.df.loc[:,self.df.columns.str.startswith('img')].values)\n",
    "        self.txt = torch.Tensor(self.df.loc[:,self.df.columns.str.startswith('text')].values)\n",
    "\n",
    "        if opt_lambda == None:\n",
    "            sales, self.opt_lambda = boxcox(self.df['sales_total'])\n",
    "        else:\n",
    "            self.opt_lambda = opt_lambda\n",
    "            sales = boxcox(self.df['sales_total'], opt_lambda)\n",
    "        self.y = torch.Tensor(sales)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fabric = self.fab[idx]\n",
    "        category = self.cat[idx]\n",
    "        color = self.col[idx]\n",
    "        image = self.img[idx]\n",
    "        text = self.txt[idx]\n",
    "        sales_total = self.y[idx]\n",
    "\n",
    "        return fabric, category, color, image, text, sales_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticFeatureEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, use_img, use_text, num_layers, before_meta, dropout=0.2):\n",
    "        super(StaticFeatureEncoder, self).__init__()\n",
    "\n",
    "        self.img_linear = nn.Linear(2048, embedding_dim)\n",
    "        self.text_linear = nn.Linear(768, embedding_dim)\n",
    "        self.use_img = use_img\n",
    "        self.use_text = use_text\n",
    "\n",
    "        self.before_meta = before_meta\n",
    "\n",
    "        self.linear = nn.Linear(535, 512)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(hidden_dim, max_len=512)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4, dropout=0.2)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "    # def forward(self, img_encoding, text_encoding):\n",
    "    def forward(self, image, text, meta_data=None):\n",
    "        # Fuse static features together\n",
    "        img_encoding = self.img_linear(image)\n",
    "        text_encoding = self.text_linear(text)\n",
    "\n",
    "        # Build input\n",
    "        decoder_inputs = []\n",
    "        if self.use_img == 1:\n",
    "            decoder_inputs.append(img_encoding)\n",
    "        if self.use_text == 1:\n",
    "            decoder_inputs.append(text_encoding)\n",
    "        if self.before_meta:\n",
    "            decoder_inputs.append(meta_data)\n",
    "\n",
    "        concat_features = torch.cat(decoder_inputs, dim=1)\n",
    "\n",
    "        if self.before_meta:\n",
    "            concat_features = self.linear(concat_features)\n",
    "\n",
    "        print(concat_features.size())\n",
    "        features = self.batchnorm(concat_features)\n",
    "        features = self.feature_fusion(features.unsqueeze(2))\n",
    "        features = self.pos_embedding(features)\n",
    "        features = self.encoder(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class Scale_factor_layer_encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, before_meta, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3, stride=2, bias=True)\n",
    "        self.conv2 = nn.Conv2d(1, 1, 3, stride=2, bias=True)\n",
    "        self.conv3 = nn.Conv2d(1, 1, 3, stride=2, bias=True)\n",
    "        self.conv4 = nn.Conv2d(1, 1, 3, stride=2, bias=True)\n",
    "        self.conv5 = nn.Conv2d(1, 1, 3, stride=2, bias=True)\n",
    "        self.conv6 = nn.Conv2d(1, 1, 3, stride=2, bias=True)\n",
    "\n",
    "\n",
    "        self.meta_linear1 = nn.Linear(23, 49, bias=True)\n",
    "\n",
    "        self.before_meta = before_meta\n",
    "\n",
    "        if before_meta:\n",
    "            self.linear = nn.Linear(49, 1, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(98, 1, bias=True)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "        # self.activation = nn.ReLU()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(255)\n",
    "        self.norm2 = nn.LayerNorm(127)\n",
    "        self.norm3 = nn.LayerNorm(63)\n",
    "        self.norm4 = nn.LayerNorm(31)\n",
    "        self.norm5 = nn.LayerNorm(15)\n",
    "        self.norm6 = nn.LayerNorm(7)\n",
    "\n",
    "        # self.pool = nn.AdaptiveAvgPool2d((64, 64))\n",
    "\n",
    "    # def forward(self, encoder_out, meta_data):\n",
    "    def forward(self, encoder_out, meta_data=None):\n",
    "        out = self.norm1(self.conv1(encoder_out))\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.norm3(self.conv3(out))\n",
    "        out = self.norm4(self.conv4(out))\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.norm5(self.conv5(out))\n",
    "        out = self.norm6(self.conv6(out))\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "\n",
    "        out = out.squeeze().reshape(out.shape[0], -1)\n",
    "\n",
    "        if self.before_meta:\n",
    "           pass\n",
    "        else:\n",
    "            out_meta = self.activation(self.meta_linear1(meta_data))\n",
    "            out = torch.cat([out, out_meta], dim=1)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class OnlyStaticFeature(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, use_img, use_text, num_layers, before_meta):\n",
    "        super().__init__()\n",
    "        self.static_feature_encoder = StaticFeatureEncoder(embedding_dim, hidden_dim, use_img, use_text, num_layers, before_meta)\n",
    "        self.scale_factor_layer_encoder = Scale_factor_layer_encoder(hidden_dim, before_meta, dropout=0.2)\n",
    "    \n",
    "    def forward(self, image, text, meta_data):\n",
    "        static_feature_fusion = self.static_feature_encoder(image, text)\n",
    "        output = self.scale_factor_layer_encoder(static_feature_fusion.unsqueeze(1), meta_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smart01/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.regression import R2Score, SymmetricMeanAbsolutePercentageError\n",
    "\n",
    "r2score = R2Score()\n",
    "smape = SymmetricMeanAbsolutePercentageError()\n",
    "device =  torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = OnlyStaticFeature(\n",
    "    args.embedding_dim, \n",
    "    args.hidden_dim, \n",
    "    args.use_img, \n",
    "    args.use_text, \n",
    "    args.num_hidden_layers, \n",
    "    args.before_meta\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "train_dataset = MetaDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset = MetaDataset(test_df, train_dataset.opt_lambda)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(target, prediction, opt_lambda):\n",
    "    prediction = inv_boxcox(prediction.detach().cpu(), opt_lambda)\n",
    "    target = inv_boxcox(target.detach().cpu(), opt_lambda)\n",
    "    \n",
    "    r2_score = torch.mean(torch.stack(\n",
    "        [r2score(prediction[i], target[i]) for i in\n",
    "            range(len(target))]))\n",
    "\n",
    "    ad_smape = torch.mean(torch.stack(\n",
    "        [smape(prediction[i], target[i]) * 0.5 for i\n",
    "            in range(len(target))]))\n",
    "\n",
    "    return r2_score, ad_smape\n",
    "\n",
    "\n",
    "def train(model, train_loader, device):\n",
    "    model.train()\n",
    "    total_mse = 0\n",
    "    total_rs = 0\n",
    "    total_smp = 0\n",
    "\n",
    "    for train_batch in train_loader:\n",
    "        fabric, category, color, image, text, sale = train_batch\n",
    "        meta = torch.cat([fabric, category, color], axis=1)\n",
    "\n",
    "        image = image.to(device)\n",
    "        text = text.to(device)\n",
    "        meta = meta.to(device)\n",
    "        sale = sale.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(image, text, meta).squeeze()\n",
    "        loss = F.mse_loss(sale, pred)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        r2_score, ad_smape = get_metric(sale.unsqueeze(0), pred.unsqueeze(0), train_loader.dataset.opt_lambda)\n",
    "        mse = loss.item()\n",
    "\n",
    "        total_mse += sale.shape[0] * mse\n",
    "        total_rs += sale.shape[0] * r2_score\n",
    "        total_smp += sale.shape[0] * ad_smape\n",
    "        \n",
    "    mse = total_mse / len(train_loader.dataset)\n",
    "    r2_score = total_rs / len(train_loader.dataset)\n",
    "    ad_smape = total_smp / len(train_loader.dataset)\n",
    "\n",
    "    print(f'Train\\t| r2_score: {r2_score:.4f}\\tad_smape: {ad_smape:.4f}\\tmse: {mse:.4f}')\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_mse = 0\n",
    "    total_rs = 0\n",
    "    total_smp = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_loader:\n",
    "            fabric, category, color, image, text, sale = test_batch\n",
    "            meta = torch.cat([fabric, category, color], axis=1)\n",
    "\n",
    "            image = image.to(device)\n",
    "            text = text.to(device)\n",
    "            meta = meta.to(device)\n",
    "            sale = sale.to(device)\n",
    "\n",
    "            pred = model(image, text, meta).squeeze()\n",
    "            loss = F.mse_loss(sale, pred)\n",
    "\n",
    "            r2_score, ad_smape = get_metric(sale.unsqueeze(0), pred.unsqueeze(0), test_loader.dataset.opt_lambda)\n",
    "            mse = loss.item()\n",
    "\n",
    "            total_mse += sale.shape[0] * mse\n",
    "            total_rs += sale.shape[0] * r2_score\n",
    "            total_smp += sale.shape[0] * ad_smape\n",
    "\n",
    "    \n",
    "    mse = total_mse / len(test_loader.dataset)\n",
    "    r2_score = total_rs / len(test_loader.dataset)\n",
    "    ad_smape = total_smp / len(test_loader.dataset)\n",
    "\n",
    "    print(f'Test\\t| r2_score: {r2_score:.4f}\\tad_smape: {ad_smape:.4f}\\tmse: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     test(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m sale \u001b[38;5;241m=\u001b[39m sale\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(sale, pred)\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 133\u001b[0m, in \u001b[0;36mOnlyStaticFeature.forward\u001b[0;34m(self, image, text, meta_data)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, text, meta_data):\n\u001b[0;32m--> 133\u001b[0m     static_feature_fusion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_feature_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_factor_layer_encoder(static_feature_fusion\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), meta_data)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m, in \u001b[0;36mStaticFeatureEncoder.forward\u001b[0;34m(self, image, text, meta_data)\u001b[0m\n\u001b[1;32m     46\u001b[0m     concat_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(concat_features)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(concat_features\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 49\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_fusion(features\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     51\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(features)\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/functional.py:2476\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2464\u001b[0m         batch_norm,\n\u001b[1;32m   2465\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2474\u001b[0m     )\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2476\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/bonbak/lib/python3.10/site-packages/torch/nn/functional.py:2444\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2442\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs+1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    train(model, train_loader, device)\n",
    "    test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonbak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
